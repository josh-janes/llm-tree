{
  "nodes": [
    {
      "id": "transformers-2017",
      "name": "Transformers",
      "date": "2017-06-01",
      "link": "https://arxiv.org/abs/1706.03762",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "The landmark paper 'Attention Is All You Need' introduced the Transformer architecture, using self-attention to process sequences in parallel. This breakthrough enabled more efficient training and set the stage for the many large-scale models that followed."
      }
    },
    {
      "id": "gpt1-2018",
      "name": "GPT 1.0",
      "date": "2018-06-01",
      "link": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "OpenAI’s GPT-1 demonstrated that a transformer-based model pre-trained on a large corpus and then fine-tuned for specific tasks could perform well across diverse language tasks. It laid the groundwork for subsequent generations of generative models."
      }
    },
    {
      "id": "bert-2018",
      "name": "BERT",
      "date": "2018-10-01",
      "link": "https://arxiv.org/abs/1810.04805",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "Developed by Google, BERT (Bidirectional Encoder Representations from Transformers) introduced bidirectional training, meaning it looks at context from both left and right. This innovation led to significant improvements in language understanding and many state-of-the-art results on NLP benchmarks."
      }
    },
    {
      "id": "gpt2-2019",
      "name": "GPT 2.0",
      "date": "2019-02-01",
      "link": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "GPT-2 scaled up the transformer approach to 1.5 billion parameters, showcasing impressive text generation and general language understanding abilities. Its release sparked important discussions around the ethical implications of powerful text-generation models."
      }
    },
    {
      "id": "megatron-lm-2019",
      "name": "Megatron-LM",
      "date": "2019-09-01",
      "link": "https://arxiv.org/abs/1909.08053",
      "image": "nvidia.png",
      "properties": {
        "organization": "NVIDIA",
        "description": "Megatron-LM, from NVIDIA, is a framework for efficiently training extremely large transformer models using optimized model and data parallelism techniques. It has been instrumental in exploring the upper limits of model scaling."
      }
    },
    {
      "id": "t5-2019",
      "name": "T5",
      "date": "2019-10-01",
      "link": "https://arxiv.org/abs/1910.10683",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "Google's T5 (Text-To-Text Transfer Transformer) recast all NLP tasks into a unified text-to-text format. This elegant approach not only simplified the architecture but also achieved state-of-the-art performance across a wide range of tasks."
      }
    },
    {
      "id": "zero-2019",
      "name": "ZeRO",
      "date": "2019-10-01",
      "link": "https://arxiv.org/abs/1910.02054",
      "image": "microsoft.png",
      "properties": {
        "organization": "Microsoft",
        "description": "ZeRO (Zero Redundancy Optimizer) is a breakthrough in training large-scale deep learning models, especially those with trillions of parameters. It focuses on optimizing memory usage during training, allowing much larger models to be trained efficiently and at higher speeds, overcoming the limitations of traditional data- and model-parallelism strategies."
      }
    },
    {
      "id": "scaling-law-2020",
      "name": "Scaling Law",
      "date": "2020-01-01",
      "link": "https://arxiv.org/abs/2001.08361",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "Research on scaling laws quantitatively demonstrated how model performance improves predictably as a function of parameters, data, and compute. These insights have been crucial for guiding the development of ever-larger language models."
      }
    },
    {
      "id": "gpt3-2020",
      "name": "GPT 3.0",
      "date": "2020-05-01",
      "link": "https://arxiv.org/abs/2005.14165",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "OpenAI's GPT-3, with 175 billion parameters, showcased remarkable few-shot and even zero-shot learning capabilities. Its ability to perform a wide array of language tasks without task-specific fine-tuning made it a milestone in the field."
      }
    },
    {
      "id": "switch-transformers-2021",
      "name": "Switch Transformers",
      "date": "2021-01-01",
      "link": "https://arxiv.org/abs/2101.03961",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "Switch Transformers introduced a mixture-of-experts approach, where only a subset of the model’s parameters are activated for any given input. This design allows models to scale to trillions of parameters while keeping computational costs manageable."
      }
    },
    {
      "id": "codex-2021",
      "name": "Codex",
      "date": "2021-08-01",
      "link": "https://arxiv.org/abs/2107.03374",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "Built on the GPT-3 architecture, Codex is fine-tuned specifically for programming tasks. It powers tools like GitHub Copilot by translating natural language into code, demonstrating strong abilities in code synthesis and understanding."
      }
    },
    {
      "id": "foundation-models-2021",
      "name": "Foundation Models",
      "date": "2021-08-01",
      "link": "https://arxiv.org/abs/2108.07258",
      "image": "stanford.png",
      "properties": {
        "organization": "Stanford",
        "description": "This term broadly describes large pre-trained models that serve as the basis for a wide variety of downstream applications. Their versatility and transferability have made them central to modern AI research and development."
      }
    },
    {
      "id": "flan-2021",
      "name": "FLAN",
      "date": "2021-09-01",
      "link": "https://arxiv.org/abs/2109.01652",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "FLAN (Fine-tuned LAnguage Net) is a family of models that are tuned on a variety of instructions to better follow human prompts. This approach has improved the model's performance on diverse tasks by aligning outputs more closely with user intent."
      }
    },
    {
      "id": "t0-2021",
      "name": "T0",
      "date": "2021-10-01",
      "link": "https://arxiv.org/abs/2110.08207",
      "image": "huggingface.svg",
      "properties": {
        "organization": "HuggingFace et al.",
        "description": "T0 is an instruction-tuned version of T5 that was trained on a diverse set of tasks. Its design emphasizes robust performance in zero-shot and few-shot scenarios, making it effective at generalizing to new instructions."
      }
    },
    {
      "id": "glam-2021",
      "name": "GLaM",
      "date": "2021-12-01",
      "link": "https://arxiv.org/abs/2112.06905",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "MoE Architecture"
      }
    },
    {
      "id": "webgpt-2021",
      "name": "WebGPT",
      "date": "2021-12-01",
      "link": "https://arxiv.org/abs/2112.09332",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "WebGPT is an experimental model by OpenAI that incorporates web-browsing capabilities to fetch up-to-date information. This integration of retrieval mechanisms helps improve factuality and relevance in its responses."
      }
    },
    {
      "id": "retro-2021",
      "name": "Retro",
      "date": "2021-12-01",
      "link": "https://arxiv.org/abs/2112.04426",
      "image": "deepmind.webp",
      "properties": {
        "organization": "DeepMind",
        "description": "RETRO (Retrieval-Enhanced Transformer) augments the standard transformer architecture with a retrieval component that brings in external information. This design helps the model produce more accurate and contextually grounded outputs."
      }
    },
    {
      "id": "gopher-2021",
      "name": "Gopher",
      "date": "2021-12-01",
      "link": "https://arxiv.org/abs/2112.11446",
      "image": "deepmind.webp",
      "properties": {
        "organization": "DeepMind",
        "description": "Developed by DeepMind, Gopher is a 280B parameter model evaluated extensively across academic and practical benchmarks. It’s particularly noted for its capabilities in reading comprehension and dialogue, contributing valuable insights into model scaling."
      }
    },
    {
      "id": "cot-2022",
      "name": "COT",
      "date": "2022-01-01",
      "link": "https://arxiv.org/abs/2201.11903",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "‘Chain-of-Thought’ (CoT) prompting techniques were introduced around 2022 to improve complex reasoning in language models. By encouraging step-by-step problem solving, CoT helps models break down and solve intricate tasks more effectively."
      }
    },
    {
      "id": "lamda-2022",
      "name": "LaMDA",
      "date": "2022-01-01",
      "link": "https://arxiv.org/abs/2201.08239",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "LaMDA (Language Model for Dialogue Applications) by Google is tailored for natural and engaging conversational AI. It emphasizes nuanced dialogue generation and safe interaction, aiming to understand and respond to human conversation in a natural manner."
      }
    },
    {
      "id": "minerva-2022",
      "name": "Minerva",
      "date": "2022-01-01",
      "link": "https://arxiv.org/abs/2206.14858",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "Minerva is a model fine-tuned for mathematical and scientific problem solving. It’s designed to handle complex quantitative reasoning tasks, making it particularly useful for academic and research-oriented applications."
      }
    },
    {
      "id": "megatron-turing-2022",
      "name": "Megatron-Turing NLG",
      "date": "2022-01-01",
      "link": "https://arxiv.org/abs/2201.11990",
      "image": "nvidia.png",
      "properties": {
        "organization": "Microsoft&NVIDIA",
        "description": "Megatron-Turing NLG is the result of a collaboration that scaled transformer architectures to 530B parameters. This model pushes the envelope in natural language understanding and generation, combining advanced training techniques from NVIDIA and Microsoft."
      }
    },
    {
      "id": "instructgpt-2022",
      "name": "InstructGPT",
      "date": "2022-03-01",
      "link": "https://arxiv.org/abs/2203.02155",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "InstructGPT represents OpenAI’s efforts to align language models with user instructions. Using reinforcement learning from human feedback (RLHF), it produces outputs that are more helpful and aligned with human expectations."
      }
    },
    {
      "id": "palm-2022",
      "name": "PaLM",
      "date": "2022-04-01",
      "link": "https://arxiv.org/abs/2204.02311",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "Google's PaLM (Pathways Language Model) is a massive model with around 540 billion parameters. It excels in multi-task learning and few-shot performance, demonstrating robust capabilities across a wide range of language tasks."
      }
    },
    {
      "id": "chinchilla-2022",
      "name": "Chinchilla",
      "date": "2022-04-01",
      "link": "https://arxiv.org/abs/2203.15556",
      "image": "deepmind.webp",
      "properties": {
        "organization": "DeepMind",
        "description": "Chinchilla, from DeepMind, emphasizes efficient scaling by striking a balance between model size and the amount of training data. The research showed that smaller models trained on more data can outperform larger models that are overtrained."
      }
    },
    {
      "id": "opt-2022",
      "name": "OPT",
      "date": "2022-05-01",
      "link": "https://arxiv.org/abs/2205.01068",
      "image": "meta.png",
      "properties": {
        "organization": "Meta",
        "description": "Meta’s OPT (Open Pre-trained Transformer) series offers a range of model sizes with an emphasis on transparency. Released with detailed training logs and model cards, OPT provides valuable resources for the research community."
      }
    },
    {
      "id": "ul2-2022",
      "name": "UL2",
      "date": "2022-05-01",
      "link": "https://arxiv.org/abs/2205.05131",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "UL2 (Unifying Language Learning) is designed to merge multiple pre-training objectives into one framework. This unified approach allows the model to perform robustly across diverse NLP tasks by integrating various learning paradigms."
      }
    },
    {
      "id": "emergent-2022",
      "name": "Emergent Abilities",
      "date": "2022-06-01",
      "link": "https://arxiv.org/abs/2206.07682",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "This term refers to the phenomenon where large language models begin to exhibit unexpected capabilities as they scale. These emergent abilities—such as improved reasoning or creativity—often only appear when models reach a critical size."
      }
    },
    {
      "id": "Gemini",
      "name": "Gemini",
      "date": "2023-12-19",
      "link": "https://arxiv.org/abs/2312.11805",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "Google Gemini is a powerful family of AI models designed for top-tier reasoning and multimodal capabilities. Built on enhanced Transformer decoders, it efficiently processes long-context inputs (up to 32K tokens) using optimized attention mechanisms. Gemini 1 supports text, images, audio, and video seamlessly, with model sizes ranging from 1.8B to 3.25B parameters."
      }
    },
    {
      "id": "flamingo",
      "name": "Flamingo",
      "date": "2022-04-29",
      "link": "https://arxiv.org/abs/2204.14198",
      "image": "deepmind.webp",
      "properties": {
        "organization": "DeepMind",
        "description": "Flamingo is a visual-language model designed to seamlessly integrate images and videos with text, allowing it to generate natural language responses conditioned on visual content. It builds on pretrained vision and language models, linking them with a specialized Perceiver Resampler that distills visual features into a compact set of tokens. These tokens are then processed using gated cross-attention layers interleaved within a frozen large language model (LLM), ensuring stable training and strong performance. The model comes in multiple sizes, with the largest version—Flamingo-80B—based on a 70B parameter language model plus additional vision-text processing layers. It excels at few-shot learning, achieving state-of-the-art results on various image and video understanding tasks without requiring extensive fine-tuning."
      }
    },
    {
      "id": "coca",
      "name": "coca",
      "date": "2022-05-04",
      "link": "https://arxiv.org/abs/2205.01917",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "CoCa (Contrastive Captioner) is a vision-language model that integrates contrastive learning and image captioning within a unified encoder-decoder architecture. It consists of a Vision Transformer (ViT) as the image encoder and a text decoder that is divided into unimodal and multimodal components. This structure allows the model to generate text representations independently and in relation to visual data. CoCa supports both contrastive and generative learning objectives, enabling it to perform tasks such as image-text retrieval and caption generation. It is available in multiple sizes, with the largest version containing 2.1 billion parameters. The model is designed for efficiency, sharing computation between its two training objectives. It has demonstrated strong zero-shot performance on various vision-language tasks, including image classification and video understanding, without requiring task-specific fine-tuning."
      }
    },
    {
      "id": "pali",
      "name": "PaLi",
      "date": "2022-09-14",
      "link": "https://arxiv.org/abs/2209.06794",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "PaLI (Pathways Language and Image) is a multimodal model designed to handle both vision and language tasks within a unified architecture. It combines a Vision Transformer (ViT) for processing images and an mT5 encoder-decoder for text generation. The model does not use task-specific components but instead relies on text-based prompts to distinguish between tasks. PaLI is available in multiple sizes, with the largest version (PaLI-17B) containing 17 billion parameters—4 billion in the vision encoder and 13 billion in the text component. It has demonstrated strong performance on a range of tasks, including image captioning, visual question answering, and text understanding across multiple languages. Its scalability suggests potential for further improvements in vision-language integration."
      }
    },
    {
      "id": "dalle",
      "name": "DALL-E",
      "date": "2021-02-24",
      "link": "https://arxiv.org/abs/2102.12092",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "DALL·E is a deep learning model designed to generate images from text descriptions. It works by using a two-stage approach: first, a discrete variational autoencoder (dVAE) compresses high-resolution images into a 32×32 grid of image tokens, significantly reducing memory requirements. Then, a massive 12-billion parameter autoregressive transformer models the relationship between text and image tokens, generating new images by predicting pixel arrangements from text prompts. DALL·E excels at creating imaginative and coherent images, thanks to its ability to capture both fine details and broader visual structures. It has been trained on a vast dataset of text-image pairs, enabling it to generate diverse and contextually relevant visuals. While it produces strikingly realistic images, it can sometimes struggle with complex spatial relationships and compositional accuracy."
      }
    },
    {
      "id": "usm",
      "name": "USM",
      "date": "2023-03-02",
      "link": "https://arxiv.org/abs/2303.01037",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "The Universal Speech Model (USM) is a large-scale AI system designed for multilingual speech recognition and translation. Built around a 2-billion-parameter Conformer architecture—a hybrid of convolutional and transformer networks—it’s trained on a vast mix of data: 12 million hours of unlabeled audio from 300+ languages, 28 billion sentences of text in 1,140 languages, and smaller labeled datasets for fine-tuning."
      }
    },
    {
      "id": "big-bench-2022",
      "name": "BIG-bench",
      "date": "2022-06-01",
      "link": "https://arxiv.org/abs/2206.04615",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "BIG-bench is a comprehensive benchmark designed to test the limits of language models on a wide variety of tasks. It has become an important tool for evaluating the generalization and reasoning capabilities of large-scale models."
      }
    },
    {
      "id": "metalm-2022",
      "name": "METALM",
      "date": "2022-06-01",
      "link": "https://arxiv.org/abs/2206.06336",
      "image": "microsoft.png",
      "properties": {
        "organization": "Microsoft",
        "description": "MetaLM refers to a series of language models from Meta (formerly Facebook) that explore scaling and efficiency. While specifics can vary, these models contribute to the broader understanding of large-scale model training and performance."
      }
    },
    {
      "id": "sparrow-2022",
      "name": "Sparrow",
      "date": "2022-09-01",
      "link": "https://arxiv.org/abs/2209.14375",
      "image": "deepmind.webp",
      "properties": {
        "organization": "DeepMind",
        "description": "Developed by DeepMind, Sparrow is a dialogue agent that focuses on safe and informative interactions. It integrates safety constraints and factual grounding, aiming to produce helpful and accurate conversational responses."
      }
    },
    {
      "id": "flan-t5-2022",
      "name": "Flan-T5/PaLM",
      "date": "2022-10-01",
      "link": "https://arxiv.org/abs/2210.11416",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "FLAN-T5 combines the T5 text-to-text framework with instruction tuning from the FLAN family. This hybrid approach improves the model’s ability to follow directions and perform a wide array of tasks effectively."
      }
    },
    {
      "id": "glm-130b-2022",
      "name": "GLM-130B",
      "date": "2022-10-01",
      "link": "https://arxiv.org/abs/2210.02414",
      "image": "tsinghua.jpg",
      "properties": {
        "organization": "Tsinghua",
        "description": "GLM-130B is a model developed with a focus on multilingual capabilities, particularly for Chinese and English. It’s noted for its efficient training techniques and competitive performance in multilingual NLP."
      }
    },
    {
      "id": "helm-2022",
      "name": "HELM",
      "date": "2022-11-01",
      "link": "https://arxiv.org/abs/2211.09110",
      "image": "stanford.png",
      "properties": {
        "organization": "Stanford",
        "description": "HELM (Holistic Evaluation of Language Models) is not a model per se but an initiative aimed at providing comprehensive evaluations of language models. It assesses models across a variety of dimensions such as fairness, robustness, and real-world performance."
      }
    },
    {
      "id": "bloom-2022",
      "name": "BLOOM",
      "date": "2022-11-01",
      "link": "https://arxiv.org/abs/2211.05100",
      "image": "bigscience.png",
      "properties": {
        "organization": "BigScience",
        "description": "BLOOM is a collaborative, open-access 176B parameter multilingual language model developed by the BigScience initiative. With 176 billion parameters, it supports many languages and tasks, emphasizing transparency and community-driven research."
      }
    },
    {
      "id": "galactica-2022",
      "name": "Galactica",
      "date": "2022-11-01",
      "link": "https://arxiv.org/abs/2211.09085",
      "image": "meta.png",
      "properties": {
        "organization": "Meta",
        "description": "Galactica was designed to specialize in scientific literature and knowledge, aiming to assist in scientific research. However, it encountered challenges regarding factual accuracy and the handling of complex scientific content."
      }
    },
    {
      "id": "opt-iml-2022",
      "name": "OPT-IML",
      "date": "2022-12-01",
      "link": "https://arxiv.org/abs/2212.12017",
      "image": "meta.png",
      "properties": {
        "organization": "Meta",
        "description": "OPT-IML is a variant of Meta’s OPT models that incorporates instruction meta learning to better follow complex instructions. Detailed public information is limited, but it represents efforts to improve model alignment."
      }
    },
    {
      "id": "flan-2023",
      "name": "Flan 2022",
      "date": "2023-01-01",
      "link": "https://arxiv.org/abs/2301.13688",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "The FLAN series in 2022 continues to refine instruction tuning, enhancing the ability of language models to follow human prompts accurately. These improvements build on previous successes to further boost task performance."
      }
    },
    {
      "id": "llama-2023",
      "name": "LLaMA",
      "date": "2023-02-01",
      "link": "https://arxiv.org/abs/2302.13971",
      "image": "meta.png",
      "properties": {
        "organization": "Meta",
        "description": "Meta’s LLaMA (Large Language Model Meta AI) models released in 2023 offer competitive performance on various benchmarks while being made more accessible to the research community, fostering transparency and collaboration."
      }
    },
    {
      "id": "kosmos-1-2023",
      "name": "Kosmos-1",
      "date": "2023-02-01",
      "link": "https://arxiv.org/abs/2302.14045",
      "image": "microsoft.png",
      "properties": {
        "organization": "Microsoft",
        "description": "Kosmos-1 is a multi-modal model that integrates both language and vision, enabling it to understand and generate content across text and images. This cross-modal capability represents a significant step toward unified AI systems."
      }
    },
    {
      "id": "lru-2023",
      "name": "LRU",
      "date": "2023-03-01",
      "link": "https://arxiv.org/abs/2303.06349",
      "image": "deepmind.webp",
      "properties": {
        "organization": "DeepMind",
        "description": "Recurrent Architecture"
      }
    },
    {
      "id": "palm-e-2023",
      "name": "PaLM-E",
      "date": "2023-03-01",
      "link": "https://arxiv.org/abs/2303.03378",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "PaLM-E is an extension of Google's PaLM model designed for embodied tasks, integrating visual perception and language understanding. This multimodal approach aims to bridge the gap between language processing and physical interaction."
      }
    },
    {
      "id": "gpt4-2023",
      "name": "GPT 4",
      "date": "2023-03-01",
      "link": "https://arxiv.org/abs/2303.08774",
      "image": "openai.jpg",
      "properties": {
        "organization": "OpenAI",
        "description": "OpenAI's GPT-4 is a flagship model known for its improved reasoning, multimodal abilities (handling both text and images), and enhanced alignment and safety features. It represents a significant leap forward in language model capabilities."
      }
    },
    {
      "id": "llava-2023",
      "name": "LLaVA",
      "date": "2023-04-01",
      "link": "https://arxiv.org/abs/2304.08485",
      "image": "microsoft.png",
      "properties": {
        "organization": "UW–Madison&Microsoft",
        "description": "LLaVA extends the LLaMA model with visual understanding, enabling it to process and generate multimodal content. This integration allows the model to combine visual context with language processing for richer interactions."
      }
    },
    {
      "id": "pythia-2023",
      "name": "Pythia",
      "date": "2023-04-01",
      "link": "https://arxiv.org/abs/2304.01373",
      "image": "eleutherai.webp",
      "properties": {
        "organization": "EleutherAI et al.",
        "description": "Pythia is a series of models released by EleutherAI that emphasizes transparency and reproducibility in research. By offering various model sizes and detailed training logs, it allows researchers to study the behavior and scaling of large language models."
      }
    },
    {
      "id": "dromedary-2023",
      "name": "Dromedary",
      "date": "2023-05-01",
      "link": "https://arxiv.org/abs/2305.03047",
      "image": "cmu.jpg",
      "properties": {
        "organization": "CMU et al.",
        "description": "Self-Alignment"
      }
    },
    {
      "id": "palm2-2023",
      "name": "PaLM 2",
      "date": "2023-05-01",
      "link": "https://arxiv.org/abs/2305.10403",
      "image": "google.png",
      "properties": {
        "organization": "Google",
        "description": "PaLM 2 is Google’s improved version of its original PaLM model. It offers better performance, refined reasoning abilities, and enhanced safety features, reflecting ongoing advancements in scaling and model alignment."
      }
    },
    {
      "id": "rwkv-2023",
      "name": "RWKV",
      "date": "2023-05-01",
      "link": "https://arxiv.org/abs/2305.13048",
      "image": "eleutherai.webp",
      "properties": {
        "organization": "EleutherAI et al.",
        "description": "RWKV offers an alternative to traditional transformers by blending aspects of recurrent neural networks (RNNs) with transformer-level performance. This novel approach to sequence modeling aims to provide efficiency and scalability."
      }
    },
    {
      "id": "dpo-2023",
      "name": "DPO",
      "date": "2023-05-01",
      "link": "https://arxiv.org/abs/2305.18290",
      "image": "stanford.png",
      "properties": {
        "organization": "Stanford",
        "description": "DPO (Direct Preference Optimization) is a method designed to align language models more directly with human preferences. By streamlining the reinforcement learning process, it seeks to produce outputs that better reflect user intentions."
      }
    },
    {
      "id": "tot-2023",
      "name": "ToT",
      "date": "2023-05-01",
      "link": "https://arxiv.org/abs/2305.10601",
      "image": "google.png",
      "properties": {
        "organization": "Google&Princeton",
        "description": "Reasoning Framework"
      }
    },
    {
      "id": "llama2-2023",
      "name": "LLaMA2",
      "date": "2023-07-01",
      "link": "https://arxiv.org/abs/2307.09288",
      "image": "meta.png",
      "properties": {
        "organization": "Meta",
        "description": "Llama 2 is the next iteration of Meta’s LLaMA models, featuring improved instruction-following, reasoning, and safety. It’s designed for both research and commercial applications, building on the success of its predecessors."
      }
    },
    {
      "id": "mistral-7b-2023",
      "name": "Mistral 7B",
      "date": "2023-10-01",
      "link": "https://arxiv.org/abs/2310.06825",
      "image": "mistral.png",
      "properties": {
        "organization": "Mistral",
        "description": "Mistral 7B is an open-source model known for its efficiency. Despite having 7 billion parameters—a relatively modest size—it has been optimized for speed and cost-effectiveness while delivering competitive performance."
      }
    },
    {
      "id": "mamba-2023",
      "name": "Mamba",
      "date": "2023-12-01",
      "link": "https://arxiv.org/abs/2312.00752",
      "image": "cmu.jpg",
      "properties": {
        "organization": "CMU&Princeton",
        "description": "Mamba is an AI model designed to handle long sequences more efficiently than Transformers, the architecture behind most modern AI models like GPT. While Transformers rely on an attention mechanism that struggles with long sequences due to its quadratic scaling, Mamba introduces Selective State Space Models (SSMs) to achieve linear-time complexity."
      }
    },
    {
      "id": "deepseek-v2-2024",
      "name": "DeepSeek-v2",
      "date": "2024-01-01",
      "link": "https://arxiv.org/abs/2405.04434",
      "image": "deepseek.webp",
      "properties": {
        "organization": "DeepSeek",
        "description": "DeepSeek-V2 is a cutting-edge AI model designed to maximize efficiency in both training and inference while maintaining strong performance. It follows the Mixture-of-Experts (MoE) architecture, meaning that instead of using all of its parameters at once, it selectively activates only a portion of them per token, making it both powerful and cost-effective."
      }
    },
    {
      "id": "olmo-2024",
      "name": "OLMo",
      "date": "2024-02-01",
      "link": "https://arxiv.org/abs/2402.00838",
      "image": "ai2.jpg",
      "properties": {
        "organization": "Ai2",
        "description": "OLMo is a truly open-source language model designed to advance the scientific study of AI by providing full transparency into its training data, model architecture, and development process—something that is increasingly rare as major AI models become closed off behind proprietary systems."
      }
    },
    {
      "id": "mamba2-2024",
      "name": "Mamba2",
      "date": "2024-05-01",
      "link": "https://arxiv.org/abs/2405.21060",
      "image": "cmu.jpg",
      "properties": {
        "organization": "CMU&Princeton",
        "description": "This research introduces a theoretical bridge between Transformers and State-Space Models (SSMs)—two of the most important architectures in deep learning. While Transformers have dominated AI breakthroughs, SSMs (like Mamba) have recently shown they can match or even outperform Transformers, especially for efficiency in handling long sequences."
      }
    },
    {
      "id": "llama3-2024",
      "name": "Llama3",
      "date": "2024-05-01",
      "link": "https://arxiv.org/abs/2407.21783",
      "image": "meta.png",
      "properties": {
        "organization": "Meta",
        "description": "Llama 3 is a new foundation model that represents a significant leap forward in the development of multilingual, multitask AI systems. It is part of a broader trend where AI models are becoming increasingly versatile, capable of handling language, coding, reasoning, and even interacting with various tools and multimodal inputs like images, video, and speech."
      }
    },
    {
      "id": "fineweb-2024",
      "name": "FineWeb",
      "date": "2024-06-01",
      "link": "https://arxiv.org/abs/2406.17557",
      "image": "huggingface.svg",
      "properties": {
        "organization": "HuggingFace",
        "description": "FineWeb is a cutting-edge text dataset designed to improve the performance of large language models (LLMs) by offering high-quality data for pretraining. In this research, the team introduces a 15-trillion token dataset sourced from 96 snapshots of Common Crawl, which is a popular archive of web data. This dataset is shown to create better-performing LLMs than other publicly available datasets used for similar purposes, like those used to train models such as Llama 3 and Mixtral."
      }
    },
    {
      "id": "olmoe-2024",
      "name": "OLMoE",
      "date": "2024-09-01",
      "link": "https://arxiv.org/abs/2409.02060",
      "image": "ai2.jpg",
      "properties": {
        "organization": "Ai2",
        "description": "OLMoE is a state-of-the-art language model that leverages a sparse Mixture-of-Experts (MoE) architecture to achieve high performance with efficient use of computational resources. The model uses 7 billion parameters, but only activates 1 billion parameters per input token. This efficient use of resources allows the model to achieve strong performance without requiring excessive computational cost."
      }
    },
    {
      "id": "qwen2.5-2024",
      "name": "Qwen2.5",
      "date": "2024-12-01",
      "link": "https://arxiv.org/abs/2412.15115",
      "image": "alibaba.webp",
      "properties": {
        "organization": "Alibaba",
        "description": "Qwen2.5 is a series of large language models (LLMs) developed to meet a wide range of use cases, with significant improvements in both pre-training and post-training compared to previous versions. The model series offers a variety of configurations, from open-weight models to proprietary solutions for specific applications, providing flexibility for different user needs."
      }
    },
    {
      "id": "deepseek-v3-2024",
      "name": "DeepSeek-V3",
      "date": "2024-12-01",
      "link": "https://arxiv.org/html/2412.19437v1",
      "image": "deepseek.webp",
      "properties": {
        "organization": "DeepSeek",
        "description": "DeepSeek-V3 is a cutting-edge Mixture-of-Experts (MoE) language model that takes AI efficiency and performance to the next level. With 671 billion total parameters, it’s a heavy hitter in terms of scale, but its smart architecture ensures that it remains efficient in both training and inference."
      }
    },
    {
      "id": "deepseek-r1-2025",
      "name": "DeepSeek-R1",
      "date": "2025-01-01",
      "link": "https://arxiv.org/abs/2501.12948",
      "image": "deepseek.webp",
      "properties": {
        "organization": "DeepSeek",
        "description": "DeepSeek-R1 introduces a new approach to enhancing reasoning capabilities in large language models (LLMs) by leveraging reinforcement learning (RL). The model family consists of two main variants: DeepSeek-R1-Zero and DeepSeek-R1, with both demonstrating advanced reasoning skills, but with distinct approaches to training."
      }
    },
    {
      "id": "claude-3",
      "name": "Claude 3",
      "date": "2024-03-04",
      "link": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
      "image": "anthropic.webp",
      "properties": {
        "organization": "Anthropic",
        "description": "Developed by Anthropic, Claude 3 is a large language model that emphasizes safety and helpfulness. Built with constitutional AI principles and refined through iterative human feedback, it strives to produce reliable, well-aligned responses while handling complex tasks."
      }
    }
  ],

  "links": [
    { "source": "transformers-2017", "target": "gpt1-2018" },
    { "source": "transformers-2017", "target": "bert-2018" },
    { "source": "transformers-2017", "target": "t5-2019" },
    { "source": "transformers-2017", "target": "claude-3" },
    { "source": "transformers-2017", "target": "scaling-law-2020" },
    { "source": "transformers-2017", "target": "deepseek-v2-2024" },
    { "source": "foundation-models-2021", "target": "mamba-2023" },
    { "source": "gpt3-2020", "target": "palm-2022" },
    { "source": "transformers-2017", "target": "olmo-2024" },
    { "source": "bert-2018", "target": "megatron-lm-2019" },
    { "source": "gpt1-2018", "target": "gpt2-2019" },
    { "source": "gpt2-2019", "target": "gpt3-2020" },
    { "source": "scaling-law-2020", "target": "gpt3-2020" },
    { "source": "megatron-lm-2019", "target": "gpt3-2020" },
    { "source": "gpt3-2020", "target": "codex-2021" },
    { "source": "gpt3-2020", "target": "instructgpt-2022" },
    { "source": "gpt3-2020", "target": "webgpt-2021" },
    { "source": "gpt3-2020", "target": "cot-2022" },
    { "source": "cot-2022", "target": "deepseek-v3-2024" },
    { "source": "t5-2019", "target": "flan-2021" },
    { "source": "t5-2019", "target": "t0-2021" },
    { "source": "transformers-2017", "target": "switch-transformers-2021" },
    { "source": "t5-2019", "target": "ul2-2022" },
    { "source": "gpt3-2020", "target": "chinchilla-2022" },
    { "source": "gpt3-2020", "target": "bloom-2022" },
    { "source": "gpt3-2020", "target": "galactica-2022" },
    { "source": "opt-2022", "target": "opt-iml-2022" },
    { "source": "flan-2021", "target": "flan-t5-2022" },
    { "source": "palm-2022", "target": "palm2-2023" },
    { "source": "llama-2023", "target": "llama2-2023" },
    { "source": "llama2-2023", "target": "llama3-2024" },
    { "source": "mamba-2023", "target": "mamba2-2024" },
    { "source": "deepseek-v2-2024", "target": "deepseek-v3-2024" },
    { "source": "deepseek-v3-2024", "target": "deepseek-r1-2025" },
    { "source": "olmo-2024", "target": "olmoe-2024" },
    { "source": "gpt3-2020", "target": "rwkv-2023" },
    { "source": "instructgpt-2022", "target": "dromedary-2023" },
    { "source": "gpt3-2020", "target": "dpo-2023" },
    { "source": "gpt3-2020", "target": "tot-2023" },
    { "source": "kosmos-1-2023", "target": "llava-2023" },
    { "source": "llama2-2023", "target": "mistral-7b-2023" },
    { "source": "gpt3-2020", "target": "pythia-2023" },
    { "source": "flan-2021", "target": "flan-2023" },
    { "source": "switch-transformers-2021", "target": "deepseek-v2-2024" },
    { "source": "transformers-2017", "target": "zero-2019" },
    { "source": "transformers-2017", "target": "foundation-models-2021" },
    { "source": "transformers-2017", "target": "glam-2021" },
    { "source": "transformers-2017", "target": "retro-2021" },
    { "source": "transformers-2017", "target": "gopher-2021" },
    { "source": "transformers-2017", "target": "lamda-2022" },
    { "source": "gpt3-2020", "target": "minerva-2022" },
    { "source": "megatron-lm-2019", "target": "megatron-turing-2022" },
    { "source": "scaling-law-2020", "target": "emergent-2022" },
    { "source": "foundation-models-2021", "target": "big-bench-2022" },
    { "source": "zero-2019", "target": "metalm-2022" },
    { "source": "instructgpt-2022", "target": "sparrow-2022" },
    { "source": "transformers-2017", "target": "glm-130b-2022" },
    { "source": "foundation-models-2021", "target": "helm-2022" },
    { "source": "transformers-2017", "target": "opt-2022" },
    { "source": "gpt3-2020", "target": "llama-2023" },
    { "source": "gpt3-2020", "target": "kosmos-1-2023" },
    { "source": "transformers-2017", "target": "lru-2023" },
    { "source": "palm-2022", "target": "palm-e-2023" },
    { "source": "gpt3-2020", "target": "gpt4-2023" },
    { "source": "foundation-models-2021", "target": "fineweb-2024" },
    { "source": "transformers-2017", "target": "qwen2.5-2024" },
    { "source": "transformers-2017", "target": "Gemini" },
    { "source": "flamingo", "target": "Gemini" },
    { "source": "coca", "target": "Gemini" },
    { "source": "usm", "target": "Gemini" },
    { "source": "dalle", "target": "Gemini" },
    { "source": "pali", "target": "Gemini" },
    { "source": "transformers-2017", "target": "dalle" },
    { "source": "transformers-2017", "target": "coca" },
    { "source": "chinchilla-2022", "target": "flamingo" }
  ]
}
